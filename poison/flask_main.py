import os
import sys
import threading
import subprocess
from typing import Callable
from time import sleep
from sklearn.utils import shuffle
from flask import Flask, render_template, request

ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(ROOT)
from quasarnix.helpers import load_model, load_tokenizer, run_detection
from update_model import load_nl2bash, load_quasarnix, update_model, update_training_set

app = Flask(__name__)

def run_command(command):
    try:
        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        stdout, stderr = process.communicate()
        return stdout if process.returncode == 0 else stderr
    except subprocess.CalledProcessError as e:
        return f"Error executing command: {e.output}"


def model_update_thread():
    global model
    global tokenizer
    global data_nl2bash
    global data_quasarnix
    global model_update_interval
    global model_update_running

    while True:
        sleep(model_update_interval)
        if not model_update_running:
            model_update_running = True
            update_model(model, tokenizer, data_nl2bash, data_quasarnix)
            model_update_running = False
        model = load_model(os.path.join(ROOT, "quasarnix_model_data_train_mlp_finetuned.torch"))


# add a path for /solution
@app.route('/solution')
def solution():
    return render_template("working_backdoor.txt")


@app.route('/', methods=['GET', 'POST'])
def index():
    global model
    global tokenizer
    debug_result = ""
    ml_result = ""

    if request.method == 'POST':
        # Get the user input
        command_ml = request.form['command_ml']
        command_debug = request.form['command_debug']
        
        if 'run_command_ml' in request.form:
            command_ml = request.form['command_ml']
            command_ml = command_ml.replace('\n', ' ') # Escape newlines
            try:
                detected = run_detection(model, tokenizer, command_ml)
                update_training_set(command_ml, detected)
                if not detected:
                    run_command(command_ml)
                    ml_result = f"[+] Command passed ML model and executed:\n\n{command_ml}"
                else:
                    ml_result = f"[-] Command detected as malicious:\n\n{command_ml}"
            except subprocess.CalledProcessError as e:
                pass

        if 'run_command_debug' in request.form:
            command_debug = request.form['command_debug']
            try:
                debug_result = run_command(command_debug)
            except subprocess.CalledProcessError as e:
                debug_result = f"Error executing command: {e.output}"

    return render_template("index.html", ml_result=ml_result, debug_result=debug_result)

if __name__ == '__main__':
    # init
    model = load_model(os.path.join(ROOT, "quasarnix_model_data_full_mlp_adv.torch"))
    tokenizer = load_tokenizer(os.path.join(ROOT, "quasarnix_tokenizer_data_full_onehot_adv.json"))
    data_nl2bash = load_nl2bash(ROOT)
    data_quasarnix = load_quasarnix(ROOT)
    data_quasarnix = shuffle(data_quasarnix)[:10000]

    model_update_interval = 60 # seconds
    model_update_running = False

    # luanch model updates in separate thread  
    model_update_thread = threading.Thread(target=model_update_thread)
    model_update_thread.daemon = True  # Daemon thread will shut down with the main Flask app
    model_update_thread.start()

    app.run(debug=True, port=8081)
