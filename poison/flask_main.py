import os
import sys
import threading
import subprocess
import concurrent.futures
from time import sleep
from sklearn.utils import shuffle
from flask import Flask, render_template, request

ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(ROOT)
from quasarnix.helpers import load_model, load_tokenizer, run_detection
from update_model import run_update, load_nl2bash, load_quasarnix

app = Flask(__name__)

def update_training_set(command: str, malicious: bool) -> None:
    if malicious:
        with open('data_train_from_web_malicious.txt', 'a') as f:
            f.write(command + '\n')
    else:
        with open('data_train_from_web_legit.txt', 'a') as f:
            f.write(command + '\n')  # Append the command followed by a newline

def update_model(timeout: int = 60):
    global model
    global tokenizer
    global nl2bash
    global quasarnix
    while True:
        sleep(timeout)
        run_update(model, tokenizer, nl2bash, quasarnix)
        model = load_model("quasarnix_model_data_train_mlp_finetuned.torch")


def run_command(command):
    try:
        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        stdout, stderr = process.communicate()
        return stdout if process.returncode == 0 else stderr
    except subprocess.CalledProcessError as e:
        return f"Error executing command: {e.output}"

@app.route('/', methods=['GET', 'POST'])
def index():
    global model
    global tokenizer
    global executor
    debug_result = ""
    ml_result = ""

    if request.method == 'POST':
        # Get the user input
        command_ml = request.form['command_ml']
        command_debug = request.form['command_debug']
        
        if 'run_command_ml' in request.form:
            command_ml = request.form['command_ml']
            command_ml = command_ml.replace('\n', ' ') # Escape newlines
            try:
                detected = run_detection(model, tokenizer, command_ml)
                update_training_set(command_ml, detected)
                if not detected:
                    future = executor.submit(run_command, command_ml)
                    ml_result = f"[+] Command passed ML model and executed:\n\n{command_ml}"
                else:
                    ml_result = f"[-] Command detected as malicious:\n\n{command_ml}"
            except subprocess.CalledProcessError as e:
                pass

        if 'run_command_debug' in request.form:
            command_debug = request.form['command_debug']
            try:
                process = subprocess.Popen(command_debug, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                stdout, stderr = process.communicate()
                debug_result = stdout if process.returncode == 0 else stderr
                
            except subprocess.CalledProcessError as e:
                debug_result = f"Error executing command: {e.output}"

    return render_template("index.html", ml_result=ml_result, debug_result=debug_result)

if __name__ == '__main__':
    # init
    model = load_model(os.path.join(ROOT, "quasarnix_model_data_train_mlp_orig.torch"))
    tokenizer = load_tokenizer(os.path.join(ROOT, "quasarnix_tokenizer_data_train_onehot_orig.json"))
    nl2bash = load_nl2bash(ROOT)
    quasarnix = load_quasarnix(ROOT)
    quasarnix = shuffle(quasarnix)[:10000]

    # luanch model updates in separate thread    
    model_update_thread = threading.Thread(target=update_model)
    model_update_thread.daemon = True  # Daemon thread will shut down with the main Flask app
    model_update_thread.start()

    executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)

    app.run(debug=True, port=8081)
