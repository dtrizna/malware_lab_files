import torch
from torch import nn 
import numpy as np
from .preprocessors import OneHotCustomVectorizer

def sigmoid(x: np.ndarray) -> np.ndarray:
    return 1 / (1 + np.exp(-x))


class SimpleMLP(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=[32], dropout=None):
        if isinstance(hidden_dim, int):
            hidden_dim = [hidden_dim]
        
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        # Dynamically create hidden layers based on hidden_dim
        for h_dim in hidden_dim:
            layers.append(nn.Linear(prev_dim, h_dim))
            layers.append(nn.ReLU())
            if dropout:
                layers.append(nn.Dropout(dropout))
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
    

def mlp_pred_proba(
    model: SimpleMLP,
    tokenizer: OneHotCustomVectorizer,
    command: str
):
    x = tokenizer.transform([command])
    logits = model(torch.Tensor(x.toarray()).reshape(1, -1))
    logits = logits.detach().cpu().numpy()
    prob = sigmoid(logits)
    return prob
