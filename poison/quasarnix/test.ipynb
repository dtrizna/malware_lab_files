{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from numpy import array\n",
    "import json\n",
    "import os\n",
    "\n",
    "class OneHotCustomVectorizer:\n",
    "    def __init__(self, tokenizer=wordpunct_tokenize, max_features=4096):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_features = max_features\n",
    "        self.vocab = {}\n",
    "        self.__name__ = \"OneHotCustomVectorizer\"\n",
    "        \n",
    "    def fit(self, sequences):\n",
    "        tokenized_sequences = [self.tokenizer(seq.lower()) for seq in sequences]\n",
    "        all_tokens = [token for sublist in tokenized_sequences for token in sublist]\n",
    "        \n",
    "        common_tokens = [item[0] for item in Counter(all_tokens).most_common(self.max_features)]\n",
    "        self.vocab = {token: idx for idx, token in enumerate(common_tokens)}\n",
    "        return self\n",
    "\n",
    "    def save_vocab(self, vocab_file):\n",
    "        vocab_file_folder = os.path.dirname(vocab_file)\n",
    "        os.makedirs(vocab_file_folder, exist_ok=True)\n",
    "        with open(vocab_file, 'w') as f:\n",
    "            json.dump(self.vocab, f, indent=4)\n",
    "    \n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "    def tokenize(self, sequences):\n",
    "        return [self.tokenizer(seq.lower()) for seq in sequences]\n",
    "\n",
    "    def detokenize(self, idx_sequence):\n",
    "        assert self.vocab != {}, \"Vocabulary not built yet. Call fit() or load_vocab() first.\"\n",
    "        return [list(self.vocab.keys())[idx] for idx in idx_sequence]\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        assert self.vocab != {}, \"Vocabulary not built yet. Call fit() or load_vocab() first.\"\n",
    "        tokenized_sequences = self.tokenize(sequences)\n",
    "        \n",
    "        onehot_encoded = lil_matrix((len(sequences), self.max_features))\n",
    "        for idx, sequence in enumerate(tokenized_sequences):\n",
    "            for token in sequence:\n",
    "                if token in self.vocab:\n",
    "                    onehot_encoded[idx, self.vocab[token]] = 1\n",
    "                    \n",
    "        return onehot_encoded.tocsr()\n",
    "\n",
    "    def encode(self, sequences):\n",
    "        return self.transform(sequences)\n",
    "\n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)\n",
    "    \n",
    "\n",
    "class CommandTokenizer:\n",
    "    def __init__(self, tokenizer_fn=wordpunct_tokenize, vocab_size=1024, max_len=256):\n",
    "        self.tokenizer_fn = tokenizer_fn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        self.max_len = max_len\n",
    "        self.__name__ = \"CommandTokenizer\"\n",
    "        \n",
    "    def tokenize(self, commands):\n",
    "        return [self.tokenizer_fn(cmd.lower()) for cmd in commands]\n",
    "    \n",
    "    def detokenize(self, idx_sequence):\n",
    "        return [list(self.vocab.keys())[idx] for idx in idx_sequence]\n",
    "\n",
    "    def build_vocab(self, tokens_list):\n",
    "        self.vocab[self.PAD_TOKEN] = 0 # PAD_TOKEN maps to 0\n",
    "        self.vocab[self.UNK_TOKEN] = 1 # UNK_TOKEN maps to 1\n",
    "        vocab = Counter()\n",
    "        for tokens in tokens_list:\n",
    "            vocab.update(tokens)\n",
    "        vocab = dict(vocab.most_common(self.vocab_size - 2))  # -2 for the UNK_TOKEN and PAD_TOKEN\n",
    "        self.vocab.update({token: idx for idx, (token, _) in enumerate(vocab.items(), 2)})\n",
    "    \n",
    "    def dump_vocab(self, vocab_file):\n",
    "        vocab_file_folder = os.path.dirname(vocab_file)\n",
    "        os.makedirs(vocab_file_folder, exist_ok=True)\n",
    "        with open(vocab_file, 'w') as f:\n",
    "            json.dump(self.vocab, f, indent=4)\n",
    "    \n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "    \n",
    "    def encode(self, tokens_list):\n",
    "        assert self.vocab != {}, \"Vocabulary not built yet. Call build_vocab() first.\"\n",
    "        return [[self.vocab.get(token, self.vocab[self.UNK_TOKEN]) for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "    def pad(self, encoded_list):\n",
    "        padded_list = []\n",
    "        for seq in encoded_list:\n",
    "            if len(seq) > self.max_len:\n",
    "                padded_seq = seq[:self.max_len]\n",
    "            else:\n",
    "                padded_seq = seq + [0] * (self.max_len - len(seq))\n",
    "            padded_list.append(padded_seq)\n",
    "        return array(padded_list)\n",
    "\n",
    "    def transform(self, commands):\n",
    "        tokenized_commands = self.tokenize(commands)\n",
    "        encoded_commands = self.encode(tokenized_commands)\n",
    "        padded_commands = self.pad(encoded_commands)\n",
    "        return padded_commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from typing import List, Union, Callable\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "\n",
    "class CSRTensorDataset(Dataset):\n",
    "    def __init__(self, csr_data, labels=None):\n",
    "        if labels is not None:\n",
    "            assert csr_data.shape[0] == len(labels)\n",
    "        self.csr_data = csr_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_tensor(self, row):\n",
    "        assert isinstance(row, (np.ndarray, torch.Tensor)),\\\n",
    "            \"Expected row to be a numpy array or torch tensor, but got {}\".format(type(row))\n",
    "        if isinstance(row, np.ndarray):\n",
    "            data = torch.from_numpy(row).float()\n",
    "        elif isinstance(row, torch.Tensor):\n",
    "            data = row.clone().detach().float()\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csr_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csr_data[index].toarray().squeeze()  # Convert the sparse row to a dense numpy array\n",
    "        x_data = self.get_tensor(row)\n",
    "        if self.labels is not None:\n",
    "            y_data = self.get_tensor(self.labels[index])\n",
    "            return x_data, y_data\n",
    "        else:\n",
    "            return x_data,\n",
    "\n",
    "\n",
    "def create_dataloader(X, y=None, batch_size=1024, shuffle=False, workers=4):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.from_numpy(X).long()\n",
    "    if y is not None and isinstance(y, np.ndarray):\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "    # Handle csr_matrix case\n",
    "    if isinstance(X, csr_matrix):\n",
    "        dataset = CSRTensorDataset(X, y)\n",
    "    # Handle torch.Tensor case\n",
    "    elif isinstance(X, torch.Tensor):\n",
    "        dataset = TensorDataset(X, y) if y is not None else TensorDataset(X)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported type for X. Supported types are numpy arrays, torch tensors, and scipy CSR matrices.\")\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "\n",
    "def commands_to_loader(\n",
    "        cmd: List[str], \n",
    "        tokenizer: Union[CommandTokenizer, OneHotCustomVectorizer], \n",
    "        workers: int, \n",
    "        batch_size: int, \n",
    "        y: np.ndarray = None,\n",
    "        shuffle: bool = False\n",
    ") -> DataLoader:\n",
    "    \"\"\"Convert a list of commands to a DataLoader.\"\"\"\n",
    "    array = tokenizer.transform(cmd)\n",
    "    if y is None:\n",
    "        loader = create_dataloader(array, batch_size=batch_size, workers=workers, shuffle=shuffle)\n",
    "    else:\n",
    "        loader = create_dataloader(array, y, batch_size=batch_size, workers=workers, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def load_data(root, seed=33, limit=None):\n",
    "    \"\"\"\n",
    "    NOTE: \n",
    "        First shuffle the data -- to take random elements from each class.\n",
    "        limit//2 -- since there are 2 classes, so full data size is limit.\n",
    "        Second shuffle the data -- to mix the two classes.\n",
    "    \"\"\"\n",
    "    train_base_parquet_file = [x for x in os.listdir(os.path.join(root,'data/nix_shell/train_baseline.parquet/')) if x.endswith('.parquet')][0]\n",
    "    test_base_parquet_file = [x for x in os.listdir(os.path.join(root,'data/nix_shell/test_baseline.parquet/')) if x.endswith('.parquet')][0]\n",
    "    train_rvrs_parquet_file = [x for x in os.listdir(os.path.join(root,'data/nix_shell/train_rvrs.parquet/')) if x.endswith('.parquet')][0]\n",
    "    test_rvrs_parquet_file = [x for x in os.listdir(os.path.join(root,'data/nix_shell/test_rvrs.parquet/')) if x.endswith('.parquet')][0]\n",
    "\n",
    "    train_baseline_df = pd.read_parquet(os.path.join(root,'data/nix_shell/train_baseline.parquet/', train_base_parquet_file))\n",
    "    test_baseline_df = pd.read_parquet(os.path.join(root,'data/nix_shell/test_baseline.parquet/', test_base_parquet_file))\n",
    "    train_malicious_df = pd.read_parquet(os.path.join(root,'data/nix_shell/train_rvrs.parquet/', train_rvrs_parquet_file))\n",
    "    test_malicious_df = pd.read_parquet(os.path.join(root,'data/nix_shell/test_rvrs.parquet/', test_rvrs_parquet_file))\n",
    "\n",
    "    if limit is not None:\n",
    "        X_train_baseline_cmd = shuffle(train_baseline_df['cmd'].values.tolist(), random_state=seed)[:limit//2]\n",
    "        X_train_malicious_cmd = shuffle(train_malicious_df['cmd'].values.tolist(), random_state=seed)[:limit//2]\n",
    "        X_test_baseline_cmd = shuffle(test_baseline_df['cmd'].values.tolist(), random_state=seed)[:limit//2]\n",
    "        X_test_malicious_cmd = shuffle(test_malicious_df['cmd'].values.tolist(), random_state=seed)[:limit//2]\n",
    "    else:\n",
    "        X_train_baseline_cmd = train_baseline_df['cmd'].values.tolist()\n",
    "        X_train_malicious_cmd = train_malicious_df['cmd'].values.tolist()\n",
    "        X_test_baseline_cmd = test_baseline_df['cmd'].values.tolist()\n",
    "        X_test_malicious_cmd = test_malicious_df['cmd'].values.tolist()\n",
    "\n",
    "    X_train_non_shuffled = X_train_baseline_cmd + X_train_malicious_cmd\n",
    "    y_train = np.array([0] * len(X_train_baseline_cmd) + [1] * len(X_train_malicious_cmd), dtype=np.int8)\n",
    "    X_train_cmds, y_train = shuffle(X_train_non_shuffled, y_train, random_state=seed)\n",
    "\n",
    "    X_test_non_shuffled = X_test_baseline_cmd + X_test_malicious_cmd\n",
    "    y_test = np.array([0] * len(X_test_baseline_cmd) + [1] * len(X_test_malicious_cmd), dtype=np.int8)\n",
    "    X_test_cmds, y_test = shuffle(X_test_non_shuffled, y_test, random_state=seed)\n",
    "\n",
    "    return X_train_cmds, y_train, X_test_cmds, y_test, X_train_malicious_cmd, X_train_baseline_cmd, X_test_malicious_cmd, X_test_baseline_cmd\n",
    "\n",
    "\n",
    "def load_nl2bash(root):\n",
    "    with open(os.path.join(root, \"data\", \"nix_shell\", \"nl2bash.cm\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        baseline = f.readlines()\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def load_tokenizer(\n",
    "        tokenizer_type: str,\n",
    "        train_cmds: List[str],\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        tokenizer_fn: Callable = wordpunct_tokenize,\n",
    "        suffix: str = \"\",\n",
    "        logs_folder: str = \"./\"\n",
    ") -> Union[OneHotCustomVectorizer, CommandTokenizer]:\n",
    "    if \"onehot\" in tokenizer_type:\n",
    "        oh_file = os.path.join(logs_folder, f\"onehot_vocab_{vocab_size}{suffix}.pkl\")\n",
    "        if os.path.exists(oh_file):\n",
    "            print(f\"[!] Loading One-Hot encoder from '{oh_file}'...\")\n",
    "            with open(oh_file, \"rb\") as f:\n",
    "                tokenizer = pickle.load(f)\n",
    "        else:\n",
    "            tokenizer = OneHotCustomVectorizer(tokenizer=tokenizer_fn, max_features=vocab_size)\n",
    "            print(\"[*] Fitting One-Hot encoder...\")\n",
    "            now = time.time()\n",
    "            tokenizer.fit(train_cmds)\n",
    "            print(f\"[!] Fitting One-Hot encoder took: {time.time() - now:.2f}s\") # ~90s\n",
    "            with open(oh_file, \"wb\") as f:\n",
    "                pickle.dump(tokenizer, f)\n",
    "    else:\n",
    "        tokenizer = CommandTokenizer(tokenizer_fn=tokenizer_fn, vocab_size=vocab_size, max_len=max_len)\n",
    "        vocab_file = os.path.join(logs_folder, f\"wordpunct_vocab_{vocab_size}{suffix}.json\")\n",
    "        if os.path.exists(vocab_file):\n",
    "                print(f\"[!] Loading vocab from '{vocab_file}'...\")\n",
    "                tokenizer.load_vocab(vocab_file)\n",
    "        else:\n",
    "            print(\"[*] Building Tokenizer for Embedding vocab and encoding...\")\n",
    "            X_train_tokens_poisoned = tokenizer.tokenize(train_cmds)\n",
    "            tokenizer.build_vocab(X_train_tokens_poisoned)\n",
    "            tokenizer.dump_vocab(vocab_file)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# powershell collections\n",
    "def read_powershell_pan_enc(root: str):\n",
    "    file = os.path.join(root, \"data\", \"dtrizna\", \"powershell\", \"pan42\", \"ps_encodedcommand_data_clean.txt\")\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_powershell_offensive(root: str):\n",
    "    folder = os.path.join(root, \"data\", \"dtrizna\", \"powershell\", \"offensive-powershell\", \"data\")\n",
    "    for parquet_file in os.listdir(folder):\n",
    "        if parquet_file.endswith(\".parquet\"):\n",
    "            if \"train\" in parquet_file:\n",
    "                train_df = pd.read_parquet(os.path.join(folder, parquet_file))\n",
    "            elif \"dev\" in parquet_file:\n",
    "                dev_df = pd.read_parquet(os.path.join(folder, parquet_file))\n",
    "            elif \"test\" in parquet_file:\n",
    "                test_df = pd.read_parquet(os.path.join(folder, parquet_file))\n",
    "    all_pwsh = np.concatenate([train_df['code'].values, dev_df['code'].values, test_df['code'].values]).tolist()\n",
    "    return all_pwsh\n",
    "\n",
    "\n",
    "def convert_powershell_ps1_to_oneliner(script: str) -> str:\n",
    "    lines = script.splitlines()\n",
    "    lines = [line for line in lines if not line.strip().startswith(\"#\")]\n",
    "    lines = [line for line in lines if line.strip()]\n",
    "    oneliner = ' '.join('; '.join(lines).split())\n",
    "    # remove param block using regex\n",
    "    oneliner = re.sub(r\"param\\s*\\([^)]*\\)\", \"\", oneliner)\n",
    "    return oneliner.lstrip(\"; .\")\n",
    "\n",
    "def read_powershell_corpus(root: str, limit: int = None):\n",
    "    dataset = []     \n",
    "    \n",
    "    corpus_folder = os.path.join(root, \"data\", \"dtrizna\", \"powershell\", \"powershell_collection\")\n",
    "    if not os.path.exists(corpus_folder):\n",
    "        raise FileNotFoundError(f\"Corpus folder '{corpus_folder}' does not exist.\")\n",
    "    \n",
    "    l = limit if limit is not None else None\n",
    "    for dirpath, _, filenames in tqdm(os.walk(corpus_folder), desc=\"[*] Reading PowerShell corpus\", total=l):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".ps1\") or filename.endswith(\".psm1\"):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    script = f.read()\n",
    "                oneliner = convert_powershell_ps1_to_oneliner(script)\n",
    "                dataset.append(oneliner)\n",
    "\n",
    "                if limit is not None and len(dataset) >= limit:\n",
    "                    return dataset\n",
    "                \n",
    "    return dataset\n",
    "\n",
    "def read_powershell_data(root: str, seed: int = 33, split_train_test: bool = True, limit: int = None):\n",
    "    pan_pwsh = read_powershell_pan_enc(root)\n",
    "    huggingface_pwsh = read_powershell_offensive(root)\n",
    "    powershell_collection_pwsh = read_powershell_corpus(root, limit=limit)\n",
    "\n",
    "    malicious = pan_pwsh + huggingface_pwsh\n",
    "    benign = powershell_collection_pwsh\n",
    "\n",
    "    malicious_labels = np.array([1] * len(malicious), dtype=np.int8)\n",
    "    benign_labels = np.array([0] * len(benign), dtype=np.int8)\n",
    "\n",
    "    X = malicious + benign\n",
    "    y = np.concatenate([malicious_labels, benign_labels])\n",
    "    X, y = shuffle(X, y, random_state=seed)\n",
    "\n",
    "    if limit is not None:\n",
    "        X, y = X[:limit], y[:limit]\n",
    "\n",
    "    if split_train_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "def read_powershell_parquet(root: str, seed: int = 33, limit: int = None):\n",
    "    train_df = pd.read_parquet(os.path.join(root, \"data\", \"powershell\", f\"train_pwsh_seed_{seed}.parquet\"))\n",
    "    test_df = pd.read_parquet(os.path.join(root, \"data\", \"powershell\", f\"test_pwsh_seed_{seed}.parquet\"))\n",
    "    \n",
    "    X_train = train_df['code'].values.tolist()\n",
    "    y_train = train_df['label'].values\n",
    "\n",
    "    X_test = test_df['code'].values.tolist()\n",
    "    y_test = test_df['label'].values\n",
    "\n",
    "    if limit is not None:\n",
    "        X_train, y_train = X_train[:limit], y_train[:limit]\n",
    "        X_test, y_test = X_test[:limit], y_test[:limit]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn \n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=[32], dropout=None):\n",
    "        if isinstance(hidden_dim, int):\n",
    "            hidden_dim = [hidden_dim]\n",
    "        \n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Dynamically create hidden layers based on hidden_dim\n",
    "        for h_dim in hidden_dim:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def mlp_pred_proba(\n",
    "    model: SimpleMLP,\n",
    "    tokenizer: OneHotCustomVectorizer,\n",
    "    command: str\n",
    "):\n",
    "    x = tokenizer.transform(command)\n",
    "    logits = model(torch.Tensor(x.toarray()).reshape(1, -1))\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    prob = sigmoid(logits)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from torch import load\n",
    "\n",
    "def run_detection(\n",
    "        pretrained_model: SimpleMLP,\n",
    "        tokenizer: OneHotCustomVectorizer,\n",
    "        command: str,\n",
    "        threshold: float = 0.5\n",
    ") -> bool:\n",
    "    prob_malicious = mlp_pred_proba(\n",
    "        model=pretrained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        commands=[command]\n",
    "    )\n",
    "    return prob_malicious > threshold # TODO: check if dims are ok\n",
    "\n",
    "def load_tokenizer(path: str) -> OneHotCustomVectorizer:\n",
    "    tokenizer_url = r\"https://huggingface.co/dtrizna/QuasarNix/raw/main/quasarnix_tokenizer_data_train_onehot_orig.json\"\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(requests.get(tokenizer_url).content)\n",
    "    oh_tokenizer_orig = OneHotCustomVectorizer()\n",
    "    oh_tokenizer_orig.load_vocab(path)\n",
    "    return oh_tokenizer_orig\n",
    "\n",
    "def load_model(path: str) -> SimpleMLP:\n",
    "    model = SimpleMLP(input_dim=4096, output_dim=1, hidden_dim=[64, 32], dropout=0.5)\n",
    "    if not os.path.exists(path):\n",
    "        mlp_model_path_url = r\"https://huggingface.co/dtrizna/QuasarNix/resolve/main/quasarnix_model_data_train_mlp_orig.torch\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(requests.get(mlp_model_path_url).content)\n",
    "        \n",
    "    state_dict = load(path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"quasarnix_model_data_train_mlp_orig.torch\")    \n",
    "tokenizer = load_tokenizer(\"quasarnix_tokenizer_data_train_onehot_orig.json\")\n",
    "command_ml = \"id\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
